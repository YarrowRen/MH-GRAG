# MultiHead与Fairness

根据给出的内容，RAG模型的公平性评估框架包含三个主要步骤：数据构建、RAG管道执行和性能评估。

### 1. 数据构建：Scenario-Based QA Construction
   - **使用的数据集**：使用TREC 2022和BBQ数据集，分别用于公平性和偏见测试，关注性别和种族等受保护群体。
   - **问题和选项构建**：根据查询和相关项目构建问题，每个问题都带有“受保护群体”和“非受保护群体”的答案选项，以及“Both”“Neither”和“不足以判断”等中立选项。
   - **四种场景**：构建了四种场景问题，用于评估系统在不同条件下的偏见表现：
     - **S1**：包含来自所有群体的相关项，测试模型能否平等识别相关性。
     - **S2**：仅包含无关项，评估模型是否会对群体有偏向。
     - **S3**：包含受保护群体的相关项和非受保护群体的无关项，测试是否倾向非受保护群体。
     - **S4**：包含受保护群体的无关项和非受保护群体的相关项，测试对非受保护群体的偏好。

### 2. RAG管道：RAG Pipeline
   - **构建方法**：从FlashRAG工具集中选择适用于评估的RAG方法，确保方法未在特定数据集上微调，以减少偏见。
   - **RAG方法**：
     - **Zero-Shot**：基于纯语言模型生成答案，不依赖外部知识。
     - **Naive**：直接使用检索到的文档，未进行优化，观察未经处理知识的影响。
     - **Selective-Context**：通过优化选择最相关的检索上下文，平衡公平性与准确性。
     - **SKR**：增强判断组件，控制是否检索文档，以分析选择性检索对公平性的影响。
     - **FLARE**：在生成期间优化检索决策，提高生成结果的公平性。
     - **Iter-RetGen**：迭代检索和生成，利用检索增强生成的方式，提升RAG模型表现。
   - **流程**：通过检索器、优化器（如Refiner）、判断器（如Judger）、生成器组件生成最终答案。

### 3. 性能评估：Performance Evaluation
   - **实用性（Utility）**：用准确率（EM）或Rouge-1衡量模型回答的正确性。
   - **公平性（Fairness）**：采用“群体差异”（Group Disparity）和“平等机会”（Equalized Odds）等指标评估模型是否存在偏见。
   - **检索表现**：通过MRR@K等指标，衡量检索阶段的质量，确保检索到的内容能够准确反映查询意图。

该框架通过控制问题构建、RAG管道优化及公平性评价，为评估RAG模型在不同群体下的偏见和公平性提供了系统化的方式。


# 使用Multihop-RAG数据集应该如何实现？

### 步骤 1：构建性别区分的知识库
1. **提取人物信息**：从corpus中识别出包含人物信息的内容，例如人物名字和相关描述。
2. **按性别分类**：将这些人物按性别分成“男性”和“女性”两个组，以便在后续测试中针对不同性别进行评估。

### 步骤 2：设计问题
在已有的QA数据集的基础上，通过对问题进行简单修改，增加涉及性别偏见的测试。您可以设计两类问题：
1. **男性偏向的问题**：在问题中偏向使用男性信息作为默认答案。例如，“谁在加密行业受到法律指控？”可以使用男性和女性人物分别作为正确和错误答案，测试模型是否倾向选择男性。
2. **女性偏向的问题**：相反，设置女性作为默认答案，测试模型在相关情境下是否更偏向选择女性。

### 步骤 3：设置公平性评估场景
基于性别偏见的评估，可以设置以下几种场景：
1. **S1（性别平等场景）**：提供男性和女性的相关信息，观察模型是否对两者的选择无偏向。
2. **S2（单一性别场景）**：提供一个性别的相关信息和另一个性别的无关信息，评估模型是否仍然能选择正确答案而不依赖性别偏见。

### 步骤 4：使用RAG评估
使用不同的RAG方法，测试模型在这些性别相关问题上的回答情况。评估指标可以包括准确率（是否选择正确的性别）以及公平性（不同性别之间的选择差异）。

通过这样的操作，可以有效评估RAG模型在性别问题上的公平性，以及检索与生成模块是否存在性别偏见。