from collections.abc import Set
from math import exp
from random import choices, shuffle
from typing import TypeVar, List

from copy import deepcopy
import networkx as nx
from networkx import Graph

from sklearn.metrics import adjusted_mutual_info_score

from .quality_functions import QualityFunction
from .utils import DataKeys as Keys
from .utils import Partition, argmax, freeze, node_total, preprocess_graph

T = TypeVar("T")


def move_nodes_fast_parallel(
    G: Graph,
    partitions: List[Partition[T]],
    ğ“—: QualityFunction[T],
    Î»: float
) -> List[Partition[T]]:
    """
    å¯¹å¤šä¸ªåˆ†åŒºï¼ˆå¤´ï¼‰åŒæ—¶æ‰§è¡Œå¿«é€Ÿå±€éƒ¨èŠ‚ç‚¹ç§»åŠ¨ï¼Œè€ƒè™‘äº’ä¿¡æ¯æ­£åˆ™åŒ–ã€‚
    
    å‚æ•°
    ----------
    G : Graph
        è¦å¤„ç†çš„å›¾/ç½‘ç»œã€‚
    partitions : List[Partition[T]]
        å½“å‰æ­£åœ¨ä¼˜åŒ–çš„åˆ†åŒºåˆ—è¡¨ã€‚
    ğ“— : QualityFunction[T]
        è¦ä¼˜åŒ–çš„è´¨é‡å‡½æ•°ã€‚
    Î» : float
        äº’ä¿¡æ¯æ­£åˆ™åŒ–çš„æƒé‡ã€‚
        
    è¿”å›
    -------
    List[Partition[T]]
        èŠ‚ç‚¹ç§»åŠ¨åçš„æ›´æ–°åˆ†åŒºåˆ—è¡¨ã€‚
    """
    Q = list(G.nodes)
    shuffle(Q)
    
    num_heads = len(partitions)
    visited_nodes = [set() for _ in range(num_heads)]
    while True:
        if not Q:
            break
        v = Q.pop(0)
        for i in range(num_heads):
            if v in visited_nodes[i]:
                continue
            visited_nodes[i].add(v)
            
            # è®¡ç®—èŠ‚ç‚¹vç§»åŠ¨åˆ°ç›®æ ‡ç¤¾åŒºçš„äº’ä¿¡æ¯æƒ©ç½š
            def mutual_info_penalty(target_community, head_index):
                penalty = 0
                for j in range(num_heads):
                    if j != head_index:
                        penalty += mutual_info_single_node(
                            partitions[head_index],
                            partitions[j],
                            v,
                            target_community
                        )
                return penalty
            
            # å¯»æ‰¾èŠ‚ç‚¹våœ¨ç¬¬iä¸ªå¤´ä¸­çš„æœ€ä½³ç¤¾åŒº
            adj_communities = list(partitions[i].adjacent_communities(v)) + [set()]
            (Câ‚˜, ğ›¥ğ“—, _) = argmax(
                lambda C: ğ“—.delta(partitions[i], v, C) - Î» * mutual_info_penalty(C, i),
                adj_communities
            )
            
            if ğ›¥ğ“— > 0:
                partitions[i].move_node(v, Câ‚˜)
                N = {u for u in G[v] if u not in Câ‚˜ and u not in visited_nodes[i]}
                Q.extend(N - set(Q))
    return partitions



def refine_partition(G: Graph, ğ“Ÿ: Partition[T], ğ“—: QualityFunction[T], Î¸: float, Î³: float) -> Partition[T]:
    """Refine all communities by merging repeatedly, starting from a singleton partition."""
    # Assign each node to its own community
    ğ“Ÿáµ£: Partition[T] = Partition.singleton_partition(G, Keys.WEIGHT)

    # Visit all communities
    for C in ğ“Ÿ:
        # refine community
        ğ“Ÿáµ£ = merge_nodes_subset(G, ğ“Ÿáµ£, ğ“—, Î¸, Î³, C)

    return ğ“Ÿáµ£


def merge_nodes_subset(G: Graph, ğ“Ÿ: Partition[T], ğ“—: QualityFunction[T], Î¸: float, Î³: float, S: Set[T]) -> Partition[T]:
    """Merge the nodes in the subset S into one or more sets to refine the partition ğ“Ÿ."""
    size_s = node_total(G, S)

    R = {
        v for v in S
          if nx.cut_size(G, [v], S - {v}, weight=Keys.WEIGHT) >= Î³ * node_total(G, v) * (size_s - node_total(G, v))
    }  # fmt: skip

    for v in R:
        # If v is in a singleton community, i.e. is a node that has not yet been merged
        if len(ğ“Ÿ.node_community(v)) == 1:
            # Consider only well-connected communities
            ğ“£ = freeze([
                C for C in ğ“Ÿ
                  if C <= S and nx.cut_size(G, C, S - C, weight=Keys.WEIGHT) >= Î³ * float(node_total(G, C) * (size_s - node_total(G, C)))
            ])  # fmt: skip

            # Now, choose a random community to put v into
            # We use python's random.choices for the weighted choice, as this is easiest.

            # Have a list of pairs of communities in ğ“£ together with the improvement (ğ›¥ğ“—) of moving v to the community
            # Only consider communities for which the quality function doesn't degrade, if v is moved there
            communities = [(C, ğ›¥ğ“—) for (C, ğ›¥ğ“—) in ((C, ğ“—.delta(ğ“Ÿ, v, C)) for C in ğ“£) if ğ›¥ğ“— >= 0]
            # Calculate the weights for the random choice using the ğ›¥ğ“— values
            weights = [exp(ğ›¥ğ“— / Î¸) for (C, ğ›¥ğ“—) in communities]

            # Finally, choose the new community
            # Use [0][0] to extract the community, since choices returns a list, containing a single (C, ğ›¥ğ“—) tuple
            Câ‚™ = choices(communities, weights=weights, k=1)[0][0]

            # And move v there
            ğ“Ÿ.move_node(v, Câ‚™)

    return ğ“Ÿ




def mutual_info(partition1: Partition[T], partition2: Partition[T]) -> float:
    """
    Calculate the mutual information between two partitions using scikit-learn's adjusted_mutual_info_score.
    
    Parameters
    ----------
    partition1 : Partition[T]
        The first partition of nodes into communities.
    partition2 : Partition[T]
        The second partition of nodes into communities.
    
    Returns
    -------
    float
        The adjusted mutual information score between the two partitions.
    """
    # è·å–æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„ç¤¾åŒºæ ‡ç­¾
    labels1 = []
    labels2 = []
    
    # å°†æ¯ä¸ªèŠ‚ç‚¹æ˜ å°„åˆ°å…¶å¯¹åº”çš„ç¤¾åŒºæ ‡ç­¾
    node_to_label1 = {node: i for i, community in enumerate(partition1.communities) for node in community}
    node_to_label2 = {node: i for i, community in enumerate(partition2.communities) for node in community}
    
    # å¯¹æ¯ä¸ªèŠ‚ç‚¹çš„æ ‡ç­¾è¿›è¡Œæ’åºï¼Œä½¿å¾—å®ƒä»¬æŒ‰ç…§ç›¸åŒçš„é¡ºåºè¿›è¡Œæ¯”è¾ƒ
    for node in partition1.G.nodes():
        labels1.append(node_to_label1[node])
        labels2.append(node_to_label2[node])
    # ä½¿ç”¨ scikit-learn è®¡ç®—è°ƒæ•´åçš„äº’ä¿¡æ¯åˆ†æ•°
    return adjusted_mutual_info_score(labels1, labels2)


def mutual_info_single_node(
    partition1: Partition[T],
    partition2: Partition[T],
    node: T,
    target_community: Set[T]
) -> float:
    """
    è®¡ç®—å°†èŠ‚ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡ç¤¾åŒºåï¼Œä¸å¦ä¸€ä¸ªåˆ†åŒºçš„äº’ä¿¡æ¯å˜åŒ–ã€‚
    
    å‚æ•°
    ----------
    partition1 : Partition[T]
        èŠ‚ç‚¹æ­£åœ¨ç§»åŠ¨çš„åˆ†åŒºã€‚
    partition2 : Partition[T]
        ç”¨äºæ¯”è¾ƒçš„å¦ä¸€ä¸ªåˆ†åŒºã€‚
    node : T
        æ­£åœ¨ç§»åŠ¨çš„èŠ‚ç‚¹ã€‚
    target_community : Set[T]
        partition1ä¸­çš„ç›®æ ‡ç¤¾åŒºã€‚
        
    è¿”å›
    -------
    float
        äº’ä¿¡æ¯çš„å˜åŒ–ã€‚
    """
    # ç”±äºäº’ä¿¡æ¯æ˜¯å…¨å±€åº¦é‡ï¼Œç²¾ç¡®è®¡ç®—å•ä¸ªèŠ‚ç‚¹ç§»åŠ¨çš„å½±å“è¾ƒä¸ºå¤æ‚ã€‚
    # è¿™é‡Œè¿‘ä¼¼åœ°é€šè¿‡åœ¨èŠ‚ç‚¹ç§»åŠ¨åé‡æ–°è®¡ç®—äº’ä¿¡æ¯æ¥ä¼°è®¡å˜åŒ–é‡ã€‚
    # æ³¨æ„ï¼šè¿™åœ¨è®¡ç®—ä¸Šå¯èƒ½ä¼šæ¯”è¾ƒè€—æ—¶ï¼Œéœ€è¦ä¼˜åŒ–ã€‚
    
    # åˆ›å»ºpartition1çš„å‰¯æœ¬å¹¶ç§»åŠ¨èŠ‚ç‚¹
    partition1_temp = deepcopy(partition1)
    partition1_temp.move_node(node, target_community)
    
    # è®¡ç®—ç§»åŠ¨åpartition1_tempä¸partition2ä¹‹é—´çš„äº’ä¿¡æ¯
    return mutual_info(partition1_temp, partition2) - mutual_info(partition1, partition2)



import pandas as pd
import os

def calculate_and_store_metrics(partitions, ğ“—, filename="metrics.csv"):
    """
    è®¡ç®—æ‰€æœ‰å¤´çš„äº’ä¿¡æ¯å’Œè´¨é‡å‡½æ•°å€¼ï¼Œå¹¶å°†ç»“æœè¿½åŠ å­˜å‚¨åˆ°CSVæ–‡ä»¶ä¸­ã€‚
    
    å‚æ•°
    ----------
    partitions : List[Partition[T]]
        å¤šä¸ªå¤´çš„ç¤¾åŒºåˆ’åˆ†ã€‚
    ğ“— : QualityFunction[T]
        è´¨é‡å‡½æ•°å®ä¾‹ï¼Œç”¨äºè®¡ç®—ç¤¾åŒºåˆ’åˆ†çš„è´¨é‡ã€‚
    filename : str
        å­˜å‚¨ç»“æœçš„CSVæ–‡ä»¶åï¼Œé»˜è®¤æ˜¯ 'metrics.csv'ã€‚
    """
    num_heads = len(partitions)
    results = []

    # è®¡ç®—æ¯ä¸ªå¤´çš„è´¨é‡å‡½æ•°
    for i in range(num_heads):
        quality = ğ“—(partitions[i])  # ç›´æ¥è°ƒç”¨ğ“—æ¥è®¡ç®—Modularity
        result_row = {'head': i, 'quality_function': quality, 'mutual_info': None}
        results.append(result_row)
    
    # è®¡ç®—å¤´ä¹‹é—´çš„äº’ä¿¡æ¯
    for i in range(num_heads):
        for j in range(i + 1, num_heads):
            mi = mutual_info(partitions[i], partitions[j])
            result_row = {'head': f'{i}-{j}', 'quality_function': None, 'mutual_info': mi}
            results.append(result_row)

    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(results)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨ï¼Œå†³å®šæ˜¯å†™å…¥æ–°æ–‡ä»¶è¿˜æ˜¯è¿½åŠ 
    if not os.path.isfile(filename):
        df.to_csv(filename, index=False)  # å†™å…¥æ–°æ–‡ä»¶
    else:
        df.to_csv(filename, mode='a', header=False, index=False)  # è¿½åŠ å†™å…¥
    print(f"Results appended to {filename}")




def multi_head_leiden_with_mutual_info_parallel(
    G: Graph, ğ“—: QualityFunction[T], num_heads: int, Î»: float, Î¸: float = 0.3, Î³: float = 0.05, weight: str | None = None
) -> List[Partition[T]]:
    """
    ä½¿ç”¨å¤šä¸ªå¤´å’Œäº’ä¿¡æ¯æ­£åˆ™åŒ–å¹¶è¡Œæ‰§è¡ŒLeidenç®—æ³•ã€‚
    
    å‚æ•°
    ----------
    G : Graph
        è¦å¤„ç†çš„å›¾/ç½‘ç»œã€‚
    ğ“— : QualityFunction[T]
        è¦ä¼˜åŒ–çš„è´¨é‡å‡½æ•°ã€‚
    num_heads: int
        è¦ç”Ÿæˆçš„ä¸åŒåˆ†åŒºçš„å¤´æ•°ã€‚
    Î» : float
        äº’ä¿¡æ¯åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡ã€‚
    Î¸ : float, optional
        Leidenæ–¹æ³•çš„Î¸å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º0.3ã€‚
    Î³ : float, optional
        Leidenæ–¹æ³•çš„Î³å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º0.05ã€‚
    weight: str | None
        å›¾ä¸­çš„è¾¹æƒé‡å±æ€§ï¼Œé»˜è®¤ä¸ºNoneã€‚
    
    è¿”å›
    -------
    List[Partition[T]]
        ç”Ÿæˆçš„ä¸åŒå¤´çš„åˆ†åŒºåˆ—è¡¨ï¼Œåº”ç”¨äº†äº’ä¿¡æ¯æ­£åˆ™åŒ–ã€‚
    """
    # é¢„å¤„ç†å›¾
    G = preprocess_graph(G, weight)
    
    # åˆå§‹åŒ–æ‰€æœ‰å¤´çš„åˆ†åŒº
    partitions = [Partition.singleton_partition(G, Keys.WEIGHT) for _ in range(num_heads)]
    
    # è®°å½•ä¹‹å‰çš„åˆ†åŒºä»¥æ£€æŸ¥æ”¶æ•›æ€§
    previous_partitions = [None for _ in range(num_heads)]
    
    while True:
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†åŒºçš„èŠ‚ç‚¹ç§»åŠ¨
        partitions = move_nodes_fast_parallel(G, partitions, ğ“—, Î»)
        
        # è®¡ç®—å¹¶å­˜å‚¨å½“å‰å¤´çš„äº’ä¿¡æ¯å’Œè´¨é‡å‡½æ•°åˆ°CSV
        calculate_and_store_metrics(partitions, ğ“—, "leiden_parallel_log.csv")  # è°ƒç”¨åˆå¹¶åçš„æ–¹æ³•

        # æ£€æŸ¥æ‰€æœ‰å¤´æ˜¯å¦æ”¶æ•›
        converged = True
        for i in range(num_heads):
            if len(partitions[i]) == G.order() or partitions[i] == previous_partitions[i]:
                continue
            else:
                converged = False
                break
        
        if converged:
            # è¿”å›å±•å¼€åçš„åˆ†åŒº
            return [partition.flatten() for partition in partitions]
        
        # è®°å½•å½“å‰åˆ†åŒº
        previous_partitions = [deepcopy(partition) for partition in partitions]
        
        # ç²¾ç‚¼æ‰€æœ‰åˆ†åŒºå¹¶èšåˆå›¾
        refined_partitions = []
        for partition in partitions:
            refined_partition = refine_partition(G, partition, ğ“—, Î¸, Î³)
            refined_partitions.append(refined_partition)
        
        # åŸºäºç¬¬ä¸€ä¸ªç²¾ç‚¼åˆ†åŒºèšåˆå›¾ï¼ˆå‡è®¾æ‰€æœ‰åˆ†åŒºå…·æœ‰ç›¸åŒçš„èŠ‚ç‚¹é›†ï¼‰
        G = refined_partitions[0].aggregate_graph()
        
        # å°†åˆ†åŒºæå‡åˆ°æ–°çš„èšåˆå›¾
        new_partitions = []
        for partition in refined_partitions:
            partitions_dict = {id: set() for id in range(len(partition))}
            for v_agg, nodes in G.nodes(data=Keys.NODES):
                community_id = partition._node_part[next(iter(nodes))]
                partitions_dict[community_id].add(v_agg)
            partitions_l = list(partitions_dict.values())
            new_partition = Partition.from_partition(G, partitions_l, Keys.WEIGHT)
            new_partitions.append(new_partition)
        partitions = new_partitions


