{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/boyuren/Documents/multi_head_graph_rag/MH-GRAG-V1/src/gnn_clustering\n",
      "/Users/boyuren/Documents/multi_head_graph_rag/MH-GRAG-V1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/Users/boyuren/Documents/multi_head_graph_rag/MH-GRAG-V1')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 1.7670\n",
      "Epoch 100, Loss: 1.7559\n",
      "Epoch 150, Loss: 1.7511\n",
      "Epoch 200, Loss: 1.7490\n",
      "Epoch 250, Loss: 1.7478\n",
      "Epoch 300, Loss: 1.7470\n",
      "Epoch 350, Loss: 1.7465\n",
      "Epoch 400, Loss: 1.7460\n",
      "Epoch 450, Loss: 1.7457\n",
      "Epoch 500, Loss: 1.7454\n",
      "Epoch 550, Loss: 1.7451\n",
      "Epoch 600, Loss: 1.7449\n",
      "Epoch 650, Loss: 1.7447\n",
      "Epoch 700, Loss: 1.7446\n",
      "Epoch 750, Loss: 1.7445\n",
      "Epoch 800, Loss: 1.7444\n",
      "Epoch 850, Loss: 1.7443\n",
      "Epoch 900, Loss: 1.7442\n",
      "Epoch 950, Loss: 1.7442\n",
      "Epoch 1000, Loss: 1.7441\n",
      "Leiden 算法的模块度: 0.8213\n",
      "Leiden 算法得到的簇数: 109\n",
      "未训练模型（初始嵌入）的 KMeans 聚类模块度: 0.2946\n",
      "KMeans 聚类的模块度: 0.7055\n",
      "随机聚类的模块度: 0.0061\n"
     ]
    }
   ],
   "source": [
    "from src.gnn_clustering.data_loader import load_data\n",
    "from src.gnn_clustering.model import get_model\n",
    "from src.gnn_clustering.loss_functions import modularity_loss\n",
    "from src.gnn_clustering.train import train_model\n",
    "from src.gnn_clustering.evaluate import (\n",
    "    get_embeddings,\n",
    "    kmeans_clustering,\n",
    "    leiden_clustering,\n",
    "    random_clustering,\n",
    "    compute_modularity,\n",
    "    format_communities\n",
    ")\n",
    "from src.gnn_clustering.utils import get_device, get_dense_adj\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 设置设备\n",
    "device = get_device()\n",
    "\n",
    "# 数据加载\n",
    "data = load_data()\n",
    "data = data.to(device)\n",
    "\n",
    "# 使用Leiden算法进行聚类，获取簇数\n",
    "communities_leiden, modularity_leiden = leiden_clustering(data)\n",
    "num_clusters_leiden = len(communities_leiden)\n",
    "\n",
    "# 获取模型\n",
    "model = get_model(data, device=device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 获取密集邻接矩阵\n",
    "adj = get_dense_adj(data.edge_index, device=device)\n",
    "\n",
    "# 初始评估\n",
    "initial_embeddings = get_embeddings(model, data, device=device)\n",
    "clusters_kmeans_initial = kmeans_clustering(initial_embeddings, n_clusters=7)\n",
    "communities_kmeans_initial = format_communities(clusters_kmeans_initial, n_clusters=7)\n",
    "modularity_kmeans_initial = compute_modularity(data, communities_kmeans_initial)\n",
    "\n",
    "# 模型训练\n",
    "model = train_model(model, data, adj, modularity_loss, optimizer)\n",
    "\n",
    "# 获取训练后的嵌入\n",
    "embeddings = get_embeddings(model, data, device=device)\n",
    "\n",
    "# KMeans 聚类评估\n",
    "clusters_kmeans = kmeans_clustering(embeddings, n_clusters=7)\n",
    "communities_kmeans = format_communities(clusters_kmeans, n_clusters=7)\n",
    "modularity_kmeans = compute_modularity(data, communities_kmeans)\n",
    "\n",
    "# # Leiden 算法评估\n",
    "# communities_leiden, modularity_leiden = leiden_clustering(data)\n",
    "# print(f'Leiden 算法的模块度: {modularity_leiden:.4f}')\n",
    "\n",
    "# 随机聚类评估\n",
    "communities_random = random_clustering(data.num_nodes, n_clusters=7)\n",
    "modularity_random = compute_modularity(data, communities_random)\n",
    "print(f'Leiden 算法的模块度: {modularity_leiden:.4f}')\n",
    "print(f'Leiden 算法得到的簇数: {num_clusters_leiden}')\n",
    "print(f'未训练模型（初始嵌入）的 KMeans 聚类模块度: {modularity_kmeans_initial:.4f}')\n",
    "print(f'KMeans 聚类的模块度: {modularity_kmeans:.4f}')\n",
    "print(f'随机聚类的模块度: {modularity_random:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.0753\n",
      "Epoch 20, Loss: 5.3945\n",
      "Epoch 40, Loss: 5.3420\n",
      "Epoch 60, Loss: 5.3230\n",
      "Epoch 80, Loss: 5.3120\n",
      "Epoch 100, Loss: 5.3041\n",
      "Epoch 120, Loss: 5.2980\n",
      "Epoch 140, Loss: 5.2931\n",
      "Epoch 160, Loss: 5.2892\n",
      "Epoch 180, Loss: 5.2861\n",
      "Epoch 200, Loss: 5.2835\n",
      "头 0 的模块度: 0.6610\n",
      "头 1 的模块度: 0.6243\n",
      "头 2 的模块度: 0.6386\n",
      "头 0 和头 1 之间的调整互信息: 0.2997\n",
      "头 0 和头 2 之间的调整互信息: 0.3313\n",
      "头 1 和头 2 之间的调整互信息: 0.3114\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from src.gnn_clustering.model import get_multi_head_model\n",
    "from src.gnn_clustering.data_loader import load_data\n",
    "from src.gnn_clustering.utils import get_device, get_dense_adj\n",
    "from src.gnn_clustering.train import train_model_multi_head\n",
    "from src.gnn_clustering.evaluate import (\n",
    "    get_embeddings_list,\n",
    "    kmeans_clustering,\n",
    "    leiden_clustering,\n",
    "    compute_modularity,\n",
    "    format_communities\n",
    ")\n",
    "# 设置设备\n",
    "device = get_device()\n",
    "\n",
    "# 数据加载\n",
    "data = load_data()\n",
    "data = data.to(device)\n",
    "\n",
    "# 使用Leiden算法进行聚类，获取簇数\n",
    "communities_leiden, modularity_leiden = leiden_clustering(data)\n",
    "num_clusters_leiden = len(communities_leiden)\n",
    "\n",
    "num_heads = 3  # 设置头的数量\n",
    "model = get_multi_head_model(data=data, device= device, num_heads=num_heads)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 获取密集邻接矩阵\n",
    "adj = get_dense_adj(data.edge_index, device=device)\n",
    "\n",
    "# 模型训练\n",
    "model = train_model_multi_head(model, data, adj, optimizer, num_heads)\n",
    "\n",
    "embeddings_list = get_embeddings_list(model,data,device)\n",
    "\n",
    "G_nx = to_networkx(data, to_undirected=True)\n",
    "\n",
    "for idx, embeddings in enumerate(embeddings_list):\n",
    "    # KMeans 聚类评估\n",
    "    clusters_kmeans = kmeans_clustering(embeddings, n_clusters=7)\n",
    "    communities_kmeans = format_communities(clusters_kmeans, n_clusters=7)\n",
    "    modularity_kmeans = compute_modularity(data, communities_kmeans)\n",
    "    print(f'头 {idx} 的模块度: {modularity_kmeans:.4f}')\n",
    "\n",
    "# 计算头之间的互信息\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "for i in range(num_heads):\n",
    "    for j in range(i + 1, num_heads):\n",
    "        clusters_i = KMeans(n_clusters=7, random_state=0).fit_predict(embeddings_list[i].cpu().numpy())\n",
    "        clusters_j = KMeans(n_clusters=7, random_state=0).fit_predict(embeddings_list[j].cpu().numpy())\n",
    "        mi = adjusted_mutual_info_score(clusters_i, clusters_j)\n",
    "        print(f'头 {i} 和头 {j} 之间的调整互信息: {mi:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.8728\n",
      "Epoch 20, Loss: 5.7533\n",
      "Epoch 40, Loss: 5.4928\n",
      "Epoch 60, Loss: 5.4233\n",
      "Epoch 80, Loss: 5.3916\n",
      "Epoch 100, Loss: 5.3719\n",
      "Epoch 120, Loss: 5.3577\n",
      "Epoch 140, Loss: 5.3488\n",
      "Epoch 160, Loss: 5.3404\n",
      "Epoch 180, Loss: 5.3355\n",
      "Epoch 200, Loss: 5.3275\n",
      "头 0 的模块度: 0.7174\n",
      "头 1 的模块度: 0.6670\n",
      "头 2 的模块度: 0.5523\n",
      "头 0 和头 1 之间的调整互信息: 0.4247\n",
      "头 0 和头 2 之间的调整互信息: 0.3555\n",
      "头 1 和头 2 之间的调整互信息: 0.3272\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.datasets import Planetoid\n",
    "# from torch_geometric.utils import to_dense_adj, to_networkx\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # 数据准备：加载 Cora 数据集\n",
    "# dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "# data = dataset[0]\n",
    "\n",
    "# # 定义单头 GNN 模型\n",
    "# class GNN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(GNN, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, 16)\n",
    "#         self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# # 定义多头 GNN 模型\n",
    "# class MultiHeadGNN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, num_heads):\n",
    "#         super(MultiHeadGNN, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.gnns = torch.nn.ModuleList([\n",
    "#             GNN(in_channels, out_channels) for _ in range(num_heads)\n",
    "#         ])\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         embeddings_list = []\n",
    "#         for gnn in self.gnns:\n",
    "#             embeddings = gnn(x, edge_index)\n",
    "#             embeddings_list.append(embeddings)\n",
    "#         return embeddings_list\n",
    "\n",
    "# # 定义模块度损失函数，包含正交正则化\n",
    "# def modularity_loss_multi_head(embeddings_list, adj, num_heads, reg_lambda=1e-3, orth_lambda=1.0):\n",
    "#     losses = []\n",
    "#     for i in range(num_heads):\n",
    "#         embeddings = embeddings_list[i]\n",
    "#         embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "#         sim = torch.mm(embeddings, embeddings.t())\n",
    "#         degrees = adj.sum(dim=1)\n",
    "#         m = adj.sum()\n",
    "#         expected = torch.outer(degrees, degrees) / m\n",
    "#         B = adj - expected\n",
    "#         modularity = (sim * B).sum() / m\n",
    "#         reg = reg_lambda * (embeddings.norm(dim=1) ** 2).sum()\n",
    "#         loss = - modularity + reg\n",
    "#         losses.append(loss)\n",
    "    \n",
    "#     orth_loss = 0\n",
    "#     for i in range(num_heads):\n",
    "#         for j in range(i + 1, num_heads):\n",
    "#             h_i = F.normalize(embeddings_list[i], p=2, dim=1)\n",
    "#             h_j = F.normalize(embeddings_list[j], p=2, dim=1)\n",
    "#             inner_product = torch.mm(h_i.t(), h_j)\n",
    "#             orth_loss += torch.norm(inner_product, p='fro') ** 2\n",
    "\n",
    "#     total_loss = sum(losses) + orth_lambda * orth_loss\n",
    "#     return total_loss\n",
    "\n",
    "# # 对抗训练函数\n",
    "# def adversarial_training(model, data, adj, num_heads, epsilon=0.01, reg_lambda=1e-3, orth_lambda=1.0):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     x_adv = data.x.clone().detach().requires_grad_(True)\n",
    "\n",
    "#     # 正常前向传播\n",
    "#     embeddings_list = model(x_adv, data.edge_index)\n",
    "#     loss = modularity_loss_multi_head(embeddings_list, adj, num_heads, reg_lambda, orth_lambda)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # 生成对抗扰动\n",
    "#     x_adv_grad = x_adv.grad.data\n",
    "#     x_adv = x_adv + epsilon * x_adv_grad.sign()\n",
    "#     x_adv = torch.clamp(x_adv, 0, 1)  # 根据特征值范围进行裁剪\n",
    "\n",
    "#     # 使用对抗样本重新计算损失\n",
    "#     optimizer.zero_grad()\n",
    "#     embeddings_list_adv = model(x_adv.detach(), data.edge_index)\n",
    "#     loss_adv = modularity_loss_multi_head(embeddings_list_adv, adj, num_heads, reg_lambda, orth_lambda)\n",
    "#     loss_adv.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     return loss_adv.item()\n",
    "\n",
    "# # 设置设备和模型\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# num_heads = 3  # 设置头的数量\n",
    "# model = MultiHeadGNN(dataset.num_features, 16, num_heads).to(device)\n",
    "# data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # 将稀疏邻接矩阵转换为密集矩阵\n",
    "# adj = to_dense_adj(data.edge_index)[0].to(device)\n",
    "\n",
    "# # 训练模型\n",
    "# for epoch in range(200):\n",
    "#     loss_value = adversarial_training(model, data, adj, num_heads, epsilon=0.01, orth_lambda=1e-8)\n",
    "#     if (epoch+1) % 20 == 0 or epoch == 0:\n",
    "#         print(f'Epoch {epoch+1}, Loss: {loss_value:.4f}')\n",
    "\n",
    "# # 测试模型并获取嵌入\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     embeddings_list = model(data.x, data.edge_index)\n",
    "#     G_nx = to_networkx(data, to_undirected=True)\n",
    "\n",
    "#     for idx, embeddings in enumerate(embeddings_list):\n",
    "#         embeddings_np = embeddings.cpu().numpy()\n",
    "#         # 使用 KMeans 进行聚类\n",
    "#         clusters = KMeans(n_clusters=7, random_state=0).fit_predict(embeddings_np)\n",
    "\n",
    "#         # 计算模块度\n",
    "#         communities = [[] for _ in range(7)]\n",
    "#         for node_idx, label in enumerate(clusters):\n",
    "#             communities[label].append(node_idx)\n",
    "#         modularity = nx.algorithms.community.modularity(G_nx, communities)\n",
    "#         print(f'头 {idx} 的模块度: {modularity:.4f}')\n",
    "\n",
    "#     # 计算头之间的互信息\n",
    "#     from sklearn.metrics import adjusted_mutual_info_score\n",
    "#     for i in range(num_heads):\n",
    "#         for j in range(i + 1, num_heads):\n",
    "#             clusters_i = KMeans(n_clusters=7, random_state=0).fit_predict(embeddings_list[i].cpu().numpy())\n",
    "#             clusters_j = KMeans(n_clusters=7, random_state=0).fit_predict(embeddings_list[j].cpu().numpy())\n",
    "#             mi = adjusted_mutual_info_score(clusters_i, clusters_j)\n",
    "#             print(f'头 {i} 和头 {j} 之间的调整互信息: {mi:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
